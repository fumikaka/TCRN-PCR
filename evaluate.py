import argparse
import torch
import os
import time
import imageio
import numpy as np
import torchvision.transforms as tfs
import sklearn.cluster as cls
import munch
import yaml

from torch.utils.data import DataLoader
from torch.optim import Adam, lr_scheduler
from torch.autograd import Variable
from datasets.trainer_dataset import build_dataset_val

from datasets.dataset_load import MultiViewDataSet, ShapeNet55

from utils import visualize_point_clouds, tdp_validate, gaussian_noise, tdp_validate_dcd, shapenet_validate_dcd,shapenet_validate

# Loading model code
from model.snowflake import TDPNet

# Transformation for ModelNet and ShapeNet
_transform = tfs.Compose([
    tfs.CenterCrop(550),
    tfs.Resize(224),
    tfs.ToTensor(),
    tfs.Normalize((.5, .5, .5), (.5, .5, .5))
])

_transform_shape = tfs.Compose([
    tfs.CenterCrop(256),
    tfs.Resize(224),
    tfs.ToTensor(),
    tfs.Normalize((.5, .5, .5), (.5, .5, .5))
])

def one_forward(model, model_name, mv, label_info=None):
    mv = np.stack(mv, axis=1).squeeze(axis=1)
    mv = torch.from_numpy(mv).float()
    mv = Variable(mv.cuda())

    syn_pc = model(mv)

    return syn_pc





def main(conf):
    # Load 3D Prototype features -- dummy
    proto_corpus = np.zeros((conf.num_prototypes, 1024), dtype=np.float)

    # Basic setting, create checkpoint folder, initialize dataset, dataloaders and models
    checkpoint_path = os.path.join(conf.model_path, opt.name)
    checkpoint_imgs = os.path.join(checkpoint_path, 'eval_images')

    if not os.path.exists(checkpoint_path):
        os.mkdir(checkpoint_path)
    if not os.path.exists(checkpoint_imgs):
        os.mkdir(checkpoint_imgs)


    model = TDPNet(conf,proto_corpus)
    # model = TDPNet(conf, 10, proto_corpus)
    # model.load_state_dict(torch.load(os.path.join(checkpoint_path, 'modelnet_airplane.pt')))
    model.load_state_dict(torch.load(os.path.join(checkpoint_path, f'{conf.name}_iter_101.pt')))

    model.cuda()
    target_label = ["airplane", "bench", "cabinet", "car", "chair", "display", "lamp", "loudspeaker",
                                    "rifle", "sofa", "table", "telephone", "vessel"]
    # target_label = ["airplane"]

    root, ply_root = conf.root, conf.proot
    # tgt_category = None if conf.cat == 'none' else conf.cat
    total = 8762
    mean_cd = .0
    mean_emd = .0
    mean_dcd = .0

    for cat in target_label:
        print(cat)

        tgt_category = cat

        config_path = conf.config
        args = munch.munchify(yaml.safe_load(open(config_path)))
        ds_loader_test, len = build_dataset_val(args, cat)
        print("length {}".format(len))

        print('Start Evaluate 2D to 3D -------------------------------------------')

        start_time = time.time()

        with torch.no_grad():
            cd, emd = shapenet_validate(model, ds_loader_test,old_version=False)
            print("Chamfer Distance  :%s" % cd)
            print("Earth Mover Distance :%s" % emd)
            # print("DCD  :%s" % dcd)
            mean_cd += cd*len/total
            mean_emd += emd*len/total
            # mean_dcd += dcd*len(mv_ds_test)/total
    print("mean cd  :%s" % mean_cd)
    print("mean emd :%s" % mean_emd)
    # print("mean_dcd  :%s" % mean_dcd)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    # Training parameters
    parser.add_argument('--batch_size', type=int, default=16, help='input batch size')
    parser.add_argument('--workers', type=int, help='number of data loading workers', default=4)
    parser.add_argument('--nepoch', type=int, default=101, help='number of epochs to train for')
    parser.add_argument('--random_seed', action="store_true", help='Fix random seed or not')
    parser.add_argument('--lrate', type=float, default=1e-4, help='learning rate')
    parser.add_argument('--lr_decay_1', type=int, default=120, help='learning rate decay 1')
    parser.add_argument('--lr_decay_2', type=int, default=140, help='learning rate decay 2')
    parser.add_argument('--lr_decay_3', type=int, default=145, help='learning rate decay 2')
    parser.add_argument('--device', type=str, default='cuda', help='Gpu usage')
    parser.add_argument('--dim_template', type=int, default=2, help='Template dimension')

    # Data
    parser.add_argument('-c', '--config', help='path to config file', default='./cfgs/svr.yaml')
    parser.add_argument('--number_points', type=int, default=2048,
                        help='Number of point sampled on the object during training, and generated by atlasnet')
    parser.add_argument('--number_points_eval', type=int, default=2048,
                        help='Number of points generated by atlasnet (rounded to the nearest squared number) ')
    parser.add_argument('--prototypes_npy', type=str, default='NOF', help='Path of the prototype npy file')

    # Save dirs and reload
    parser.add_argument('--name', type=str, default="cross", help='training name')
    parser.add_argument('--dir_name', type=str, default="", help='name of the log folder.')

    # Network
    parser.add_argument('--num_layers', type=int, default=2, help='number of hidden MLP Layer')
    parser.add_argument('--hidden_neurons', type=int, default=512, help='number of neurons in each hidden layer')
    parser.add_argument('--nb_primitives', type=int, default=32, help='number of primitives')
    parser.add_argument('--template_type', type=str, default="SQUARE", choices=["SPHERE", "SQUARE"],
                        help='dim_out_patch')
    parser.add_argument("--remove_all_batchNorms", action="store_true", help="Replace all batchnorms by identity")

    parser.add_argument('--bottleneck_size', type=int, default=1536, help='dim_out_patch')
    parser.add_argument('--activation', type=str, default='relu',
                        choices=["relu", "sigmoid", "softplus", "logsigmoid", "softsign", "tanh"], help='dim_out_patch')
    parser.add_argument('--num_prototypes', type=int, default=8, help='Number of prototypes')
    parser.add_argument('--num_slaves', type=int, default=4, help='Number of slave mlps per prototype')

    # Loss
    parser.add_argument('--no_metro', action="store_true", help='Compute metro distance')

    # Additional arguments
    parser.add_argument('--root', type=str, required=True, help='The path of multi-view dataset')
    parser.add_argument('--proot', type=str, required=True, help='The path of corresponding pc dataset')
    parser.add_argument('--cat', type=str, required=True, help='Target category')
    parser.add_argument('--model_path', type=str, default='../results/c4')

    parser.add_argument('--sample_interval', type=int, default=10, help='The gap between each sampling process')
    parser.add_argument('--save_interval', type=int, default=20, help='The gap between each model saving')

    parser.add_argument('--from_scratch', action="store_true", help='Train the point_feature_extractor from scratch')
    parser.add_argument('--reclustering', action="store_true", help='Flag that controls the re-clustering behavior')
    parser.add_argument('--dataset', type=str, default='modelnet',
                        help='The dataset to use, chose from [modelnet|shapenet]')

    opt = parser.parse_args()
    main(opt)
